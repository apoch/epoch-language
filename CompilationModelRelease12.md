# Background #

The [compilation model in Release 11](http://code.google.com/p/epoch-language/wiki/CompilationModel) has been documented for some time, and includes several notes on potential future optimizations and improvements. This page is intended to describe the work invested in Epoch Release 12 in improving the compilation model in a number of ways.


# Overview #

In Epoch Release 12, the compiler has been completely rewritten, all the way from the initial lexing/parsing phases through to code generation. Specifically, the model favors a more traditional multiple-pass approach over the ad hoc technique used in Release 11 and earlier.

  * Lexical analysis is performed by `boost::spirit::lexertl`
  * Parsing into an abstract syntax tree is performed by `boost::spirit::qi`
  * Syntactical validation is performed as part of this parsing step
  * The AST is transformed into an intermediate representation (IR) for semantic analysis
  * Semantic validation is performed on this IR once it is completely elaborated
  * Assuming semantic checking passes, code is generated by traversing the now-decorated IR


## Parser Speed Improvements ##

The replacement of `boost::spirit::classic` and associated streamlining of the lex/parse phases of compilation has achieved a documented [one-thousand-fold speedup](http://code.google.com/p/scribblings-by-apoch/wiki/OptimizingBoostSpirit) over the parse phase as implemented in Release 11. Even unoptimized debug builds show an average of two orders of magnitude speed increase. This has provided the basis for rapid iteration on both the Epoch compiler back-end itself and of programs compiled using the Release 12 compilation model.


## Syntactic Validation ##

By separating syntactical and semantic checking, we have dramatically reduced the workload expected of the parser itself, which has contributed in large part to the noteworthy speed increases of that system. In Release 12, we no longer attempt to perform any semantic checking - even type checking of literals - during parsing.

For example, all literals - even numeric and boolean literals - are stored in the AST as string tokens. (More correctly, they are stored as pairs of iterators which point into the original code text stream.) Only once the syntax of the program has been deemed correct do we bother checking to see what the _type_ of a literal is; this is performed during the later semantic validation phase. This obviates the need to type-check the literal potentially several times as the parser backtracks, which in turn removes a substantial amount of processing work from the parsing itself.

In essence, syntactical validation only ensures that a program is _superficially_ well-structured. Whether or not it makes "sense" is up to the semantic checks. A missing brace or parenthesis will cause syntax validation to fail; by contrast, passing an `integer` to a function that expects a `string` will succeed _syntax_ checks but be later rejected by _semantic_ checks.


## The Abstract Syntax Tree and Intermediate Representations ##

The AST generated by `boost::spirit::qi` makes heavy use of `boost::variant` and special "deferred construction" wrapper classes. These help make the AST generation process as fast as possible, and still succinct to express in code; however, they are far from optimal for traversing and operating on the AST data itself.

Immediately after syntax validation succeeds and a valid AST has been constructed, the compiler performs a single pass over the AST to convert it into a different data structure, known as an _intermediate representation_. The IR in this case is designed to facilitate fast semantic checking.


## Semantic Validation ##

During the conversion of the AST into the semantic IR, we accumulate metadata previously recorded during the "prepass" phase of the compiler. This includes information like the locations of function definitions, the types of each member of a structure, and so on.

Notably, it is possible to gather this metadata in a single traversal of the AST, while _simultaneously_ processing other aspects of the program's semantics. As mentioned above, literal values are converted from string tokens to their final in-memory representations during this pass, for instance. Using a simple stack-driven state machine, the semantic checker can _decorate_ the AST (or more accurately, the IR) with additional details about the program semantics as it traverses the AST itself. The net result is that an entire program can be semantically validated in a single pass over the AST, followed by a single pass over the IR.


## Type Inference ##

As of this writing, final implementation of the type inference system in the Release 12 compiler is still ongoing. It remains to be seen whether additional passes will be necessary to successfully reimplement full type inference as provided by Release 11; however, even in the worst case, multiple type inference passes can be accomplished by simply traversing the IR repeatedly until all required type information is elaborated. By virtue of not reparsing the code in every pass, this will naturally be far more efficient than the Release 11 approach.


## Code Generation ##

Following semantic validation and type inference, the program is ready for final code generation. This comprises a simple pass over the decorated IR and metadata set, where each function, structure, and so on is visited exactly once. Linear traversal of the IR directly outputs a stream of bytecode, which is suitable for immediate execution or storage in a specially crafted executable binary.

Due to the design of the compiler and virtual machine, it is not necessary to perform an explicit linkage step; instead, metadata (such as relative byte offsets of the beginnings of functions) can be either stored in the image directly or computed by the VM itself at image load time, thereby eliminating the need to adjust the output bytecode directly. Whereas traditional compilation and execution models require things like function offsets to be stored directly in the calling code (e.g. as the target of a `call` instruction in x86 assembly), the Epoch model allows functions to be called via their names, which are remapped to instruction offsets by the VM at load time. This slightly increases load time, although with the tremendous benefit of simplifying and speeding up compilation substantially. This tradeoff is considered well justified given the fact that the VM's performance itself is still a subject for future consideration.


# Implications #

While the speed and simplicity of the new compiler implementation are ample wins by themselves, there are a few noteworthy implications of this new model which we consider to be very substantial gains for the Epoch project as a whole.

Faster parsing and a clearly defined AST/IR schema will enable better tooling for the language. As a proof-of-concept, Release 12's version of the Era IDE will include a heavily improved syntax highlighting system, which will help identify user defined types, variables, functions, and so on via coloration. However, this is only the beginning.

Since the AST (and the IRs used by the compiler) do not discard any information about the input program, it is always possible to fully recover the original source (sans formatting) given an IR data set from the compiler. This provides an excellent opportunity for implementing pretty-printers and automatic formatters for the language. No longer will it be necessary to manually enforce code formatting conventions; each programmer can load the same source and view it in the formatting style of his or her choice directly in the IDE.

Another exciting possibility for this model is escaping the traditionally file-oriented concept of separate compilation. Rather than arranging Epoch programs explicitly into individual files on disk, it will be possible to store Epoch code with any arbitrary mapping between files and "modules" of code. The IDE can take care of the details of displaying this code organization in any way the programmer sees fit. Moreover, the compiler can provide feedback to help the IDE split the _actual_ code into physical units which are optimal for enabling the fastest possible rebuild times in the separate-compilation model.

One final prospect is the elimination of file granularity as a unit of measurement in processes such as revision control. Instead of having to treat each file as a monolithic chunk of code, Epoch revision control systems will be able to intelligently annotate version histories based on the _actual semantics of the program_. Moving a function implementation from one module to another no longer obliterates the revision history for that function; instead of tracking the function's code changes at the file/textual level, revision control can identify that the same function is now in a different physical location, and carry over the complete change history right alongside it.


# Conclusion #

As Release 12 nears completion and preparation for launch, we are increasingly excited about the future of Epoch - not just in terms of speed and turnaround time during development, but also in terms of heavy changes to the fundamental way we think about organizing and managing large programs. Deep integration of the Epoch compilation model into the Era IDE - including a planned rewrite of the Epoch compiler in Epoch itself, rather than the current implementation language of C++ - promises to deliver a plethora of revolutionary benefits to the workflow of developers.

Epoch was originally envisioned over five years ago as a pragmatic language to carry forward programming into the future - and not just by way of hardware support or language features. We hope to create not only a language but an entire development ecosystem that streamlines the programming process and enables new heights of innovation and enjoyment in software creation.

Thanks for joining us for the ride!